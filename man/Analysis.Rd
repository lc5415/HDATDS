% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PerformanceSearch.R
\name{Analysis}
\alias{Analysis}
\title{Perform cross-validation for a given model and given dataset}
\usage{
Analysis(
  model,
  data,
  outcome = "CVD_status",
  kfolds = 5,
  train.proportion = 0.8
)
}
\arguments{
\item{model}{Choice of model to be trained, current supported options are: "xgboost", "svm" and "glm"}

\item{data}{Data to be used for model training. Must be passed whole (not training/testing) as the splits
happens internally}

\item{outcome}{Response variable of choice. Must be one of the columns in data.}

\item{kfolds}{Number of folds for k-fold cross validation. (default = 5)}

\item{train.proportion}{Proportion of data to be kept for training (default = 0.8)}
}
\value{
A plot of the ROC curve from test data and the corresponding AUC value
}
\description{
This function performs cross validation for a given model and a given dataset and returns
the AUC (or chosen metric - not supported yet) and ROC curve for the best model found on a test set.
}
\details{
A seed is set internally for reproducibility across runs. The data is split into training/testing set
and then the training set is split into folds. The same folds are created at each run ensuring model
performance is comparable across models and not affected by random chance.
}
\examples{
data(iris)
iris = iris \%>\% dplyr::filter(Species!="virginica") \%>\%
  dplyr::mutate(Species = as.numeric(Species)-1)
Analysis("glm",iris, outcome = "Species")

}
